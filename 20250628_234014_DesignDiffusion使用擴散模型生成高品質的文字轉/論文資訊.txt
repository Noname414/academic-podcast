論文標題: DesignDiffusion：使用擴散模型生成高品質的文字轉設計圖像
研究領域: 文字轉圖像生成 (Text-to-Image Generation)
生成時間: 2025-06-28 23:40:20
==================================================

論文摘要:
本文提出了一個名為 DesignDiffusion 的框架，這是一個簡單而有效的單階段端到端擴散模型，專為從文字描述合成設計圖像這項新任務而設計。主要挑戰在於生成準確且風格一致的文字與視覺內容。現有方法通常需要預先定義文字區域或採用兩階段生成，這限制了模型的創造力，並可能導致文字與視覺元素風格不一致。為了解決此問題，DesignDiffusion 直接從使用者提示詞同步合成文字和視覺元素，無需複雜的佈局建模。該框架利用從視覺文字中提取的獨特字元嵌入來增強輸入提示，並結合字元定位損失來強化文字生成的監督。此外，我們採用自博弈直接偏好優化（SP-DPO）微調策略，以提升合成視覺文字的品質與準確性。實驗證明，DesignDiffusion 在設計圖像生成任務上達到了最先進的性能。

研究方法:
本研究提出一個基於擴散模型（Stable Diffusion XL）的端到端框架。其訓練過程分為兩階段：第一階段，透過將提示詞中的文字分解為字元級別的嵌入（Prompt Enhancement）並引入字元定位損失（Character Localization Loss）來微調模型，使其能準確地將文字渲染到圖像中。第二階段，採用自博弈直接偏好優化（SP-DPO）策略，將真實圖像視為「獲勝」數據，將模型生成且文字渲染效果差的圖像視為「失敗」數據，對模型進行進一步微調，以提升生成文字的準確性。

主要結果:
實驗結果表明，DesignDiffusion在設計圖像生成任務上取得了最先進的（state-of-the-art）性能。在定量評估中，該模型在圖像品質指標（FID分數為19.87）和文字渲染準確度（OCR精確度、召回率、F-score）方面均顯著優於現有的文字轉圖像及視覺文字渲染模型。定性比較與使用者研究也證實，DesignDiffusion生成的圖像在文字與視覺元素的整合度、佈局美學和文字準確性上，都比先前的方法更為出色與和諧。

核心創新點:
  1. 提出DesignDiffusion，一個端到端的單階段擴散框架，能同時生成圖像與視覺文字，無需預先定義文字區域或分開的生成過程。
  2. 引入提示詞增強（Prompt Enhancement）與字元定位損失（Character Localization Loss），透過字元級分解和注意力監督，顯著提升文字渲染的準確性與保真度。
  3. 採用自博弈直接偏好優化（SP-DPO）微調策略，利用真實數據作為正樣本、模型生成的劣質樣本作為負樣本，進一步提升視覺文字的準確度與整體圖像品質。
  4. 建立了一個包含一百萬張高質量設計圖像的大規模新數據集，用於模型訓練與評估。
